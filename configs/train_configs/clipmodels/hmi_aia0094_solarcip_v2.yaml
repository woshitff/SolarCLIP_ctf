model:
  base_learning_rate: 1.0e-04
  base_learning_optimizer: "Adam"
  base_learning_schedule: "CosineAnnealingLR"
  target: models.clipmodels.solarclip_v2.SolarCLIP_v2
  params:
    ckpt_path: null
    base_modal_key: 'hmi_image'
    paired_modal_key: 'aia0094_image'
    token_type: 'all embedding'
    inner_loss_rate: 0.0

    base_modal_TokenizerConfig:
    model:
      target: models.reconmodels.autoencoder.models.vqvae.models.vqgan_v1.VQTokenizer
      params:
        ckpt_path: null
        vq_modal: 'hmi_image'
        embed_dim: 8
        n_embed: 8192

        ddconfig:
          double_z: false
          z_channels: 8
          resolution: 1024
          in_channels: 1
          out_ch: 1
          ch: 32
          ch_mult: [1,2,4,4]  
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0

        lossconfig:
          target: nn.Identity

    paired_modal_TokenizerConfig:
      target: models.reconmodels.autoencoder.models.vqvae.models.vqgan_v1.VQTokenizer
      params:
        ckpt_path: null
        vq_modal: 'aia0094_image'
        embed_dim: 8
        n_embed: 8192

        ddconfig:
          double_z: false
          z_channels: 8
          resolution: 1024
          in_channels: 1
          out_ch: 1
          ch: 32
          ch_mult: [1,2,4,4]  
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0

        lossconfig:
          target: nn.Identity

    base_modal_VitConfig:
      target: models.clipmodels.modules.vit.BaseVisionTransformer
      params:
        input_dim: 8
        width: 768
        layers: 12
        heads: 8
        output_dim: 512
        token_type: 'all embedding'
        norm_type: 'bn1d'

    paired_modal_VitConfig:
      target: models.clipmodels.modules.vit.BaseVisionTransformer
      params:
        input_dim: 8
        width: 768
        layers: 12
        heads: 8
        output_dim: 512
        token_type: 'all embedding'
        norm_type: 'bn1d'


data:
  target: data.build.DataModuleFromConfig
  params:
    batch_size: 2
    num_workers: 9
    wrap: false
    train:
      target: data.dataset.SolarDataset.multimodal_dataset
      params:
        modal_list: 
          - 'magnet'
          - '0094'
        load_imgs: false
        enhance_list:
          - 1024
          - 0.5
          - 90
        time_interval: 
          - 0
          - 5346720
        time_step: 1
    validation:
      target: data.dataset.SolarDataset.multimodal_dataset
      params:
        modal_list: 
          - 'magnet'
          - '0094'
        load_imgs: false
        enhance_list:
          - 1024
          - 0.5
          - 90
        time_interval: 
          - 5346720
          - 7452000
        time_step: 115200


lightning:

  trainer:
    accelerator: "gpu"
    devices: 4
    max_epochs: 1000

  modelcheckpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      save_last: true
      every_n_epochs: 1